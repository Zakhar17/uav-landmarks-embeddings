{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkzUW16DHNuV"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C5yYD_rGKgu",
        "outputId": "dfcf75c4-52e5-44c5-fb92-1630ef84f750"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9XEJKPPGly1"
      },
      "outputs": [],
      "source": [
        "!unzip drive/MyDrive/Data/Buildings_v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr2f2pg3lFZS"
      },
      "outputs": [],
      "source": [
        "!unzip drive/MyDrive/Data/vpair_sample.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Gx2WiJHXDR",
        "outputId": "8a336cfb-df56-48ec-92a9-8de9425debdf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "len(os.listdir(\"train/images\")) + len(os.listdir(\"valid/images\")) + len(os.listdir(\"test/images\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcojmoLFuWRo"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "cities = defaultdict(lambda: 0)\n",
        "\n",
        "\n",
        "for set_ in [\"train\", \"valid\", \"test\"]:\n",
        "    imgs = os.listdir(f\"{set_}/images\")\n",
        "    for img in imgs:\n",
        "        city = img.split(\"_\")[0].split(\"-\")[0]\n",
        "        cities[city] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHdqqhkUudI5",
        "outputId": "72469002-c267-4fb7-886b-0847689f4931"
      },
      "outputs": [],
      "source": [
        "cities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcfUuSdsHSeO"
      },
      "source": [
        "# Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4s4_qk6pGZq",
        "outputId": "566ce9d0-6430-41b8-f399-e999a5f00f8e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pprint import pprint\n",
        "from IPython.display import Image, display\n",
        "\n",
        "from ultralytics import YOLO, settings\n",
        "\n",
        "\n",
        "def get_random_file(folder_path):\n",
        "    # Get list of all files in the folder\n",
        "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\"No files found in the specified directory.\")\n",
        "\n",
        "    # Select a random file\n",
        "    random_file = random.choice(files)\n",
        "\n",
        "    return os.path.join(folder_path, random_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCI6hxqmHVGc"
      },
      "source": [
        "# Work with YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNVSh0fKWMV9",
        "outputId": "b3eec1ae-338a-4ab8-bad2-c3fea0679b78"
      },
      "outputs": [],
      "source": [
        "settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szzFHOAeL7Sc"
      },
      "outputs": [],
      "source": [
        "# Disable W&B\n",
        "import os\n",
        "os.environ['WANDB_MODE'] = 'disabled'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptqNwT_IjTpB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtoIm0aFvCFs",
        "outputId": "29ff9292-3444-4106-94ed-9ddf903e154e"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"model/yolo11n-seg_modified.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset downloaded from roboflow: https://universe.roboflow.com/roboflow-universe-projects/buildings-instance-segmentation/dataset/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEoPN6ZKWC-c",
        "outputId": "9797bcb9-85cd-4dae-aacf-7041f89da6d7"
      },
      "outputs": [],
      "source": [
        "train_results = model.train(\n",
        "    data=\"data.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQl1ILmajZPo"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px9fVfN2HpN9",
        "outputId": "26c34f03-e299-4327-8e73-b0a5f9f9ab19"
      },
      "outputs": [],
      "source": [
        "metrics = model.val(\n",
        "    data=\"data.yaml\",\n",
        "    # imgsz=640\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yal5D195pqYU"
      },
      "outputs": [],
      "source": [
        "random_file = get_random_file(\"test/images\")\n",
        "results = model(random_file)\n",
        "results[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPS1KQ0UdBTZ"
      },
      "outputs": [],
      "source": [
        "Image(random_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45XmR54zdfD-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "def plot_yolo_results_from_path(\n",
        "    image_path: str,\n",
        "    model: YOLO,\n",
        "    true_segmentation_path: Optional[str] = None,\n",
        "    show_yolo_detections: bool = True,\n",
        "    show_confidence: bool = True,\n",
        "    show_labels: bool = True,\n",
        "    figsize: Tuple[int, int] = (18, 6),\n",
        "    conf_threshold: float = 0.25,\n",
        "    iou_threshold: float = 0.45\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Loads an image from the given path, optionally performs YOLO inference, optionally loads true segmentation data,\n",
        "    and plots the original image, true segmentation regions, and YOLO detections side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path (str): Path to the original image.\n",
        "    - model (YOLO): The YOLO model instance from Ultralytics YOLOv8.\n",
        "    - true_segmentation_path (Optional[str]): Path to the TXT file containing true segmentation data.\n",
        "    - show_yolo_detections (bool): Whether to perform YOLO inference and display detections.\n",
        "    - show_confidence (bool): Whether to display confidence scores on the detections.\n",
        "    - show_labels (bool): Whether to display class labels on the detections.\n",
        "    - figsize (tuple): Size of the matplotlib figure.\n",
        "    - conf_threshold (float): Confidence threshold for filtering detections.\n",
        "    - iou_threshold (float): IoU threshold for Non-Max Suppression.\n",
        "\n",
        "    Returns:\n",
        "    - None. Displays the plot.\n",
        "    \"\"\"\n",
        "    # -------------------------\n",
        "    # 1. Load the Original Image\n",
        "    # -------------------------\n",
        "    original_image = cv2.imread(image_path)\n",
        "    if original_image is None:\n",
        "        raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
        "\n",
        "    height, width = original_image.shape[:2]\n",
        "\n",
        "    # Convert original image to RGB for plotting\n",
        "    if original_image.shape[2] == 3:\n",
        "        original_plot = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        original_plot = original_image.copy()\n",
        "\n",
        "    # Initialize list to hold images and their titles\n",
        "    images = [(\"Original Image\", original_plot)]\n",
        "\n",
        "    # -------------------------\n",
        "    # 2. Load and Parse True Segmentation Data (Optional)\n",
        "    # -------------------------\n",
        "    if true_segmentation_path is not None:\n",
        "        true_segmentation_image = None\n",
        "        # Initialize a blank mask\n",
        "        true_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "        try:\n",
        "            with open(true_segmentation_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "\n",
        "            for line_num, line in enumerate(lines, start=1):\n",
        "                tokens = line.strip().split()\n",
        "                if len(tokens) < 3:\n",
        "                    print(f\"Warning: Line {line_num} has insufficient data and will be skipped.\")\n",
        "                    continue  # Not enough data to form a polygon\n",
        "\n",
        "                class_id = tokens[0]  # Assuming first token is class ID (can be used for coloring)\n",
        "                coords = tokens[1:]\n",
        "\n",
        "                if len(coords) % 2 != 0:\n",
        "                    print(f\"Warning: Line {line_num} has an odd number of coordinates and will be skipped.\")\n",
        "                    continue  # Coordinates should be in x,y pairs\n",
        "\n",
        "                # Extract x and y coordinates, assuming they are normalized (0 to 1)\n",
        "                try:\n",
        "                    x_normalized = list(map(float, coords[::2]))\n",
        "                    y_normalized = list(map(float, coords[1::2]))\n",
        "                except ValueError:\n",
        "                    print(f\"Warning: Line {line_num} contains non-float values and will be skipped.\")\n",
        "                    continue  # Skip lines with non-float values\n",
        "\n",
        "                # Convert normalized coordinates to pixel values\n",
        "                x_pixels = [int(x * width) for x in x_normalized]\n",
        "                y_pixels = [int(y * height) for y in y_normalized]\n",
        "\n",
        "                # Ensure pixel values are within image bounds\n",
        "                x_pixels = [min(max(x, 0), width - 1) for x in x_pixels]\n",
        "                y_pixels = [min(max(y, 0), height - 1) for y in y_pixels]\n",
        "\n",
        "                # Combine x and y into a list of (x, y) tuples\n",
        "                points = np.array(list(zip(x_pixels, y_pixels)), dtype=np.int32)\n",
        "\n",
        "                # Draw the polygon on the true_mask\n",
        "                cv2.fillPoly(true_mask, [points], color=255)  # White color for mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error while reading true segmentation file: {e}\")\n",
        "            true_mask = None\n",
        "\n",
        "        if true_mask is not None:\n",
        "            # Create a colored mask for visualization (e.g., blue)\n",
        "            true_mask_colored = np.zeros_like(original_image)\n",
        "            true_mask_colored[:, :, 0] = true_mask  # Blue channel\n",
        "\n",
        "            alpha = 0.4  # Transparency factor\n",
        "            true_segmentation_image = cv2.addWeighted(original_image, 1, true_mask_colored, alpha, 0)\n",
        "\n",
        "            # Convert to RGB for plotting\n",
        "            if true_segmentation_image.shape[2] == 3:\n",
        "                true_segmentation_image = cv2.cvtColor(true_segmentation_image, cv2.COLOR_BGR2RGB)\n",
        "            else:\n",
        "                true_segmentation_image = true_segmentation_image.copy()\n",
        "\n",
        "            # Append to images list\n",
        "            images.append((\"True Segmentation\", true_segmentation_image))\n",
        "\n",
        "    # -------------------------\n",
        "    # 3. Perform YOLO Inference and Annotate Image (Optional)\n",
        "    # -------------------------\n",
        "    if show_yolo_detections:\n",
        "        results = model(original_image, conf=conf_threshold, iou=iou_threshold)\n",
        "\n",
        "        if not results:\n",
        "            print(\"Warning: No results returned from the YOLO model.\")\n",
        "            annotated_plot = original_plot  # Fallback to original image\n",
        "        else:\n",
        "            # Assuming single image inference\n",
        "            detection = results[0]\n",
        "\n",
        "            # Annotate the image with detections\n",
        "            annotated_image = original_image.copy().astype(np.uint8)\n",
        "\n",
        "            # Check if masks are available\n",
        "            masks_available = detection.masks is not None and hasattr(detection.masks, 'xy') and len(detection.masks.xy) > 0\n",
        "\n",
        "            # Iterate over each detection with an index\n",
        "            for i, box in enumerate(detection.boxes):\n",
        "                # Extract bounding box coordinates and convert to integers\n",
        "                if box.xyxy is not None and box.xyxy.shape[0] > 0:\n",
        "                    x1, y1, x2, y2 = [int(coord) for coord in box.xyxy[0].tolist()]\n",
        "                else:\n",
        "                    continue  # Skip if bounding box is not available\n",
        "\n",
        "                # Extract confidence score\n",
        "                confidence = box.conf.item() if box.conf is not None else None\n",
        "\n",
        "                # Extract class ID and get the class name\n",
        "                class_id = int(box.cls.item()) if box.cls is not None else None\n",
        "                label = model.names[class_id] if class_id is not None and class_id < len(model.names) else 'Object'\n",
        "\n",
        "                # Draw bounding box in green\n",
        "                cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
        "\n",
        "                # Prepare label text\n",
        "                label_text = label\n",
        "                if show_confidence and confidence is not None:\n",
        "                    label_text += f' {confidence:.2f}'\n",
        "\n",
        "                # Put label text above the bounding box\n",
        "                if show_labels:\n",
        "                    (text_width, text_height), baseline = cv2.getTextSize(\n",
        "                        label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "                    # Ensure the text background doesn't go above the image\n",
        "                    y_label = max(y1 - text_height - baseline, 0)\n",
        "                    cv2.rectangle(\n",
        "                        annotated_image,\n",
        "                        (x1, y_label),\n",
        "                        (x1 + text_width, y_label + text_height + baseline),\n",
        "                        (0, 255, 0),\n",
        "                        thickness=cv2.FILLED\n",
        "                    )\n",
        "                    cv2.putText(\n",
        "                        annotated_image,\n",
        "                        label_text,\n",
        "                        (x1, y_label + text_height),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.5,\n",
        "                        (0, 0, 0),\n",
        "                        2\n",
        "                    )\n",
        "\n",
        "                # If segmentation masks are available and YOLO segmentation display is enabled, overlay them\n",
        "                if masks_available and i < len(detection.masks.xy):\n",
        "                    mask = detection.masks.xy[i]  # Get mask polygon points for the ith detection\n",
        "                    if mask is not None and len(mask) > 0:\n",
        "                        # Create a binary mask from polygon points\n",
        "                        mask_img = np.zeros((height, width), dtype=np.uint8)\n",
        "                        polygon = np.array(mask, dtype=np.int32)\n",
        "                        cv2.fillPoly(mask_img, [polygon], 255)\n",
        "\n",
        "                        # Create a colored mask (e.g., red)\n",
        "                        colored_mask = np.zeros_like(annotated_image)\n",
        "                        colored_mask[:, :, 2] = mask_img  # Red channel\n",
        "\n",
        "                        alpha = 0.4  # Transparency factor\n",
        "                        annotated_image = cv2.addWeighted(annotated_image, 1, colored_mask, alpha, 0)\n",
        "\n",
        "            # Convert annotated image to RGB for plotting\n",
        "            if annotated_image.shape[2] == 3:\n",
        "                annotated_plot = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "            else:\n",
        "                annotated_plot = annotated_image.copy()\n",
        "\n",
        "            # Append to images list\n",
        "            images.append((\"YOLO Detections\", annotated_plot))\n",
        "\n",
        "    # -------------------------\n",
        "    # 4. Plotting with Matplotlib\n",
        "    # -------------------------\n",
        "    num_images = len(images)\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
        "\n",
        "    # If only one subplot, axes is not a list\n",
        "    if num_images == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, (title, img) in zip(axes, images):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "frJLCZmKdxf9",
        "outputId": "550a657b-b8c2-4e2c-dc83-c1a1951df135"
      },
      "outputs": [],
      "source": [
        "image_path = get_random_file(\"test/images\")\n",
        "# image_path = get_random_file(\"vpair_sample/queries\")\n",
        "label_path = image_path.replace(\"images\", \"labels\")[:-3] + \"txt\"\n",
        "\n",
        "plot_yolo_results_from_path(\n",
        "    image_path=image_path,\n",
        "    model=model,\n",
        "    true_segmentation_path=label_path,\n",
        "    show_yolo_detections=True,\n",
        "    show_confidence=True,\n",
        "    show_labels=True,\n",
        "    figsize=(12, 6),\n",
        "    conf_threshold=0.25,\n",
        "    iou_threshold=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate strides values for embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlPOtCSF9Hpq"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"model/best.pt\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h-tX143ZQac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ShapeRecorder:\n",
        "    \"\"\"A hook class to store the output shape of a given layer.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.output_shape = None\n",
        "\n",
        "    def __call__(self, module, module_in, module_out):\n",
        "        \"\"\"\n",
        "        module_out is the output tensor of shape (B, C, H, W).\n",
        "        We store only (H, W).\n",
        "        \"\"\"\n",
        "        if isinstance(module_out, torch.Tensor):\n",
        "            # If output is a single Tensor\n",
        "            self.output_shape = module_out.shape[-2:]  # (H, W)\n",
        "        elif isinstance(module_out, (list, tuple)) and len(module_out) > 0:\n",
        "            # Some modules might return tuples/lists, e.g., (features, aux)\n",
        "            # If so, store the shape of the first tensor\n",
        "            if isinstance(module_out[0], torch.Tensor):\n",
        "                self.output_shape = module_out[0].shape[-2:]\n",
        "            else:\n",
        "                self.output_shape = None\n",
        "        else:\n",
        "            self.output_shape = None\n",
        "\n",
        "def measure_backbone_strides(yolo_model, input_size=(640, 640)):\n",
        "    \"\"\"\n",
        "    1) Hooks every submodule in yolo_model.model.model (the backbone + neck).\n",
        "    2) Runs a dummy forward pass with a (1, 3, input_size[0], input_size[1]) tensor.\n",
        "    3) Computes stride as input_height / output_height for each submodule.\n",
        "\n",
        "    Returns:\n",
        "      strides_list: a list of (layer_index, (H_out, W_out), stride_value)\n",
        "                    in the order the submodules are encountered.\n",
        "    \"\"\"\n",
        "    # 1) Convert submodules to a list so we can index them\n",
        "    modules_list = list(yolo_model.model.model.children())\n",
        "\n",
        "    # 2) Create a shape recorder + hook for each submodule\n",
        "    recorders = []\n",
        "    hooks = []\n",
        "    for module in modules_list:\n",
        "        recorder = ShapeRecorder()\n",
        "        hook = module.register_forward_hook(recorder)\n",
        "        recorders.append(recorder)\n",
        "        hooks.append(hook)\n",
        "\n",
        "    # 3) Prepare a dummy input\n",
        "    dummy_input = torch.randn(1, 3, input_size[0], input_size[1])\n",
        "\n",
        "    # 4) Perform a raw forward pass (no grad needed)\n",
        "    with torch.no_grad():\n",
        "        _ = yolo_model.model(dummy_input)\n",
        "        # Usually yolo_model.model(...) calls the backbone+neck\n",
        "        # If it raises an error, you might need to adapt:\n",
        "        #   e.g. out = yolo_model.model.model(dummy_input)\n",
        "\n",
        "    # 5) Compute strides from recorded shapes\n",
        "    strides_list = []\n",
        "    for idx, rec in enumerate(recorders):\n",
        "        out_shape = rec.output_shape  # (H_out, W_out) or None\n",
        "        if out_shape is not None:\n",
        "            H_out, W_out = out_shape\n",
        "            # Assuming the model keeps aspect ratio the same,\n",
        "            # stride = input_height / H_out = input_size[0] / H_out\n",
        "            stride_h = input_size[0] / H_out\n",
        "            stride_w = input_size[1] / W_out\n",
        "            # Typically stride_h == stride_w for square inputs and isotropic downsampling\n",
        "            stride_val = (stride_h, stride_w)\n",
        "            strides_list.append((idx, (H_out, W_out), stride_val))\n",
        "        else:\n",
        "            strides_list.append((idx, None, None))\n",
        "\n",
        "    # 6) Remove hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return strides_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ML6PJRAZVnR",
        "outputId": "ed4a1809-ee87-4833-b428-0a052aed93f4"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"model/best.pt\")\n",
        "measure_backbone_strides(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# pretty-format confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          normalize=True,\n",
        "                          save_dir=\"\",\n",
        "                          names=(),\n",
        "                          on_plot=None,\n",
        "                          text_size=12,\n",
        "                          cmap=\"Blues\"):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix with full text‐size and colormap control.\n",
        "\n",
        "    Args:\n",
        "        cm (2D list or np.ndarray): Confusion matrix; entries may be int or None.\n",
        "        normalize (bool): Normalize by column.\n",
        "        save_dir (str): If provided, directory to save the figure.\n",
        "        names (tuple): Class names for ticks.\n",
        "        on_plot (callable): Optional callback getting the saved path.\n",
        "        text_size (float or str): Font size for all text (annotations, title, axes, ticks).\n",
        "        cmap (str or Colormap): Matplotlib colormap for the matrix.\n",
        "    \"\"\"\n",
        "    original = np.array(cm, dtype=object)\n",
        "    cm_numeric = np.array([[0 if v is None else v for v in row] for row in cm], dtype=float)\n",
        "\n",
        "    # normalize\n",
        "    if normalize:\n",
        "        col_sums = cm_numeric.sum(axis=0, keepdims=True) + 1e-9\n",
        "        mat = cm_numeric / col_sums\n",
        "        mat[mat < 0.005] = np.nan\n",
        "    else:\n",
        "        mat = cm_numeric.copy()\n",
        "\n",
        "    fig, ax = plt.subplots(tight_layout=True)\n",
        "\n",
        "    # colormap\n",
        "    im = ax.imshow(mat,\n",
        "                   interpolation='nearest',\n",
        "                   cmap=cmap,\n",
        "                   vmin=0.0,\n",
        "                   vmax=np.nanmax(mat) if np.nanmax(mat) > 0 else 1)\n",
        "    cbar = fig.colorbar(im, ax=ax)\n",
        "    # make colorbar ticks readable\n",
        "    for lbl in cbar.ax.get_yticklabels():\n",
        "        lbl.set_fontsize(text_size)\n",
        "\n",
        "    # ticks & labels\n",
        "    n = cm_numeric.shape[0]\n",
        "    if names:\n",
        "        if len(names) == n:\n",
        "            ticks = list(names)\n",
        "        elif len(names) == n - 1:\n",
        "            ticks = list(names) + [\"background\"]\n",
        "        else:\n",
        "            ticks = \"auto\"\n",
        "    else:\n",
        "        ticks = \"auto\"\n",
        "\n",
        "    ax.set_xticks(np.arange(cm_numeric.shape[1]))\n",
        "    ax.set_yticks(np.arange(cm_numeric.shape[0]))\n",
        "    if ticks != \"auto\":\n",
        "        ax.set_xticklabels(ticks, fontsize=text_size)\n",
        "        ax.set_yticklabels(ticks, fontsize=text_size)\n",
        "    else:\n",
        "        ax.set_xticklabels(np.arange(cm_numeric.shape[1]), fontsize=text_size)\n",
        "        ax.set_yticklabels(np.arange(cm_numeric.shape[0]), fontsize=text_size)\n",
        "\n",
        "    # axis labels & title\n",
        "    ax.set_xlabel(\"True\", fontsize=text_size)\n",
        "    ax.set_ylabel(\"Predicted\", fontsize=text_size)\n",
        "    title = \"Confusion Matrix\" + (\" Normalized\" if normalize else \"\")\n",
        "    ax.set_title(title, fontsize=text_size)\n",
        "\n",
        "    # annotations\n",
        "    max_val = np.nanmax(mat) if np.nanmax(mat) > 0 else 1\n",
        "    thresh = max_val / 2.0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if original[i, j] is None:\n",
        "                txt = \"\"\n",
        "            elif normalize and np.isnan(mat[i, j]):\n",
        "                txt = \"\"\n",
        "            else:\n",
        "                txt = f\"{mat[i, j]:.2f}\" if normalize else f\"{cm_numeric[i, j]:.0f}\"\n",
        "            val = mat[i, j] if not (normalize and np.isnan(mat[i, j])) else 0\n",
        "            color = \"white\" if val > thresh else \"black\"\n",
        "            ax.text(j, i, txt,\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=color,\n",
        "                    fontsize=text_size)\n",
        "\n",
        "    # save if requested\n",
        "    if save_dir:\n",
        "        fname = Path(save_dir) / f'{title.lower().replace(\" \", \"_\")}.png'\n",
        "        fig.savefig(fname, dpi=250)\n",
        "        if on_plot:\n",
        "            on_plot(fname)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "confusion_matrix = [\n",
        "\t[18794, 8462],\n",
        "\t[5628,  None]\n",
        "]\n",
        "class_names = ['Building', 'Background']\n",
        "plot_confusion_matrix(\n",
        "\tconfusion_matrix,\n",
        "    normalize=False,\n",
        "    save_dir=\"\",\n",
        "    names=class_names,\n",
        "    text_size=16,\n",
        "    cmap=\"Blues\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ptqNwT_IjTpB",
        "2ZSU7KECjcW3",
        "t7X3EmzNGJGc",
        "x9jeUhmiQibm",
        "qcB4S6ycLGbk"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
